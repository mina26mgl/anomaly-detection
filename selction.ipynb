{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70018f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IMENE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "import category_encoders as ce\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import gaussian_kde\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "# from scipy.stats import zscore\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, log_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from category_encoders import CountEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "# from sklearn.ensemble import VotingAnomalyDetector\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b039b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5706697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA2.csv\", sep=';', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f97228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Isolation Forest Modifié pour Silhouette Score -----\n",
    "class IsolationTree:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    @staticmethod\n",
    "    def _c(n):\n",
    "        if n <= 1: \n",
    "            return 0\n",
    "        return 2 * (np.log(n - 1) + 0.5772) - (2 * (n - 1) / n)\n",
    "    def fit(self, X, depth=0):\n",
    "        if depth >= self.max_depth or len(X) <= 1:\n",
    "            return {\"size\": len(X)}\n",
    "        n_features = X.shape[1]\n",
    "        feature = np.random.randint(0, n_features)\n",
    "        min_val, max_val = np.min(X.iloc[:, feature]), np.max(X.iloc[:, feature])\n",
    "        if min_val == max_val:\n",
    "            return {\"size\": len(X)}\n",
    "        split = np.random.uniform(min_val, max_val)\n",
    "        left = X[X.iloc[:, feature] < split]\n",
    "        right = X[X.iloc[:, feature] >= split]\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"split\": split,\n",
    "            \"left\": self.fit(left, depth + 1),\n",
    "            \"right\": self.fit(right, depth + 1)\n",
    "        }\n",
    "    \n",
    "    def path_length(self, x, node=None, depth=0):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        if \"size\" in node:\n",
    "            return depth + self._c(node[\"size\"])\n",
    "        \n",
    "        # Make sure we don't try to access non-existent features\n",
    "        if node[\"feature\"] >= len(x):\n",
    "            return depth + self._c(1)  # Return a base case if feature is out of bounds\n",
    "            \n",
    "        value = x[node[\"feature\"]]\n",
    "\n",
    "        if value < node[\"split\"]:\n",
    "            return self.path_length(x, node[\"left\"], depth + 1)\n",
    "        else:\n",
    "            return self.path_length(x, node[\"right\"], depth + 1)\n",
    "    \n",
    "\n",
    "class IsolationForestScratch:\n",
    "    def __init__(self, n_trees=100, max_depth=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.X = None\n",
    "        self.threshold = None\n",
    "        self.scores = None\n",
    "        self.contamination = 0.05  # Default contamination rate\n",
    "    \n",
    "\n",
    "    def anomaly_score(self, X=None):\n",
    "        X = self.X if X is None else (X.values if isinstance(X, pd.DataFrame) else X)\n",
    "        scores = []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            if isinstance(x, pd.Series):  # Convert Series to numpy array if needed\n",
    "                x = x.values\n",
    "                \n",
    "            lengths = [t.path_length(x) for t in self.trees]\n",
    "            avg = np.mean(lengths)\n",
    "            score = 2 ** (-avg / IsolationTree._c(len(self.X)))  # Call through IsolationTree\n",
    "            scores.append(score)\n",
    "        return np.array(scores)\n",
    "    def fit(self, X):\n",
    "        # Convert to numpy array and ensure 2D\n",
    "        self.X = np.asarray(X)\n",
    "        if len(self.X.shape) == 1:\n",
    "            self.X = self.X.reshape(-1, 1)\n",
    "            \n",
    "        height_limit = int(np.ceil(np.log2(len(self.X))))\n",
    "        self.trees = []\n",
    "        \n",
    "        for _ in range(self.n_trees):\n",
    "            sample_idx = np.random.choice(len(self.X), len(self.X) // 2, replace=False)\n",
    "            sample = self.X[sample_idx]  # Use numpy array directly\n",
    "            tree = IsolationTree(self.max_depth or height_limit)\n",
    "            tree.tree = tree.fit(pd.DataFrame(sample))  # Convert to DataFrame only for the tree fitting\n",
    "            self.trees.append(tree)\n",
    "        \n",
    "        # Calculate threshold based on contamination\n",
    "        scores = self.anomaly_score()\n",
    "        self.threshold = np.percentile(scores, 100 * (1 - self.contamination))\n",
    "    \n",
    "    def get_distance_matrix(self):\n",
    "        \"\"\"Retourne une matrice de distance basée sur les scores d'anomalie\"\"\"\n",
    "        scores = self.anomaly_score()\n",
    "        return cdist(scores.reshape(-1, 1), scores.reshape(-1, 1))\n",
    "    \n",
    "    def get_cluster_labels(self, X, threshold=None):\n",
    "        \"\"\"\n",
    "        Returns cluster labels (0=normal, 1=anomaly) based on anomaly scores\n",
    "        If no threshold is provided, uses the one calculated during fit()\n",
    "        \"\"\"\n",
    "        scores = self.anomaly_score(X)\n",
    "        threshold = threshold or self.threshold\n",
    "        \n",
    "        if threshold is None:\n",
    "            raise ValueError(\"Threshold not set. Call fit() first or provide a threshold.\")\n",
    "            \n",
    "        return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# ----- LOF Modifié pour Silhouette Score -----\n",
    "class LOF:\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "        self.X = None\n",
    "        self.distances = None\n",
    "        self.threshold = 1.5 \n",
    "\n",
    "    def fit(self, X, contamination=0.05):\n",
    "        self.X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "        self.distances = cdist(self.X, self.X)\n",
    "        \n",
    "        # Calculate threshold based on contamination\n",
    "        scores = self.anomaly_score()\n",
    "        self.threshold = np.percentile(scores, 100 * (1 - contamination))\n",
    "\n",
    "    def _reach_dist(self, i, j):\n",
    "        dist = self.distances[i, j]\n",
    "        k_dist_j = np.partition(self.distances[j], self.k)[self.k]\n",
    "        return max(dist, k_dist_j)\n",
    "\n",
    "    def _lrd(self, i):\n",
    "        neighbors = np.argsort(self.distances[i])[1:self.k+1]\n",
    "        reach_dists = [self._reach_dist(i, j) for j in neighbors]\n",
    "        return 1 / (np.mean(reach_dists) + 1e-10)\n",
    "\n",
    "    def anomaly_score(self, X=None):\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "            distances = cdist(X, self.X)\n",
    "        \n",
    "        lrd_scores_train = [self._lrd(i) for i in range(len(self.X))]\n",
    "        scores = []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if X is self.X:\n",
    "                neighbors = np.argsort(self.distances[i])[1:self.k+1]\n",
    "            else:\n",
    "                neighbors = np.argsort(distances[i])[:self.k]\n",
    "            \n",
    "            lrd_x = 1 / (np.mean([\n",
    "                max(distances[i][j] if X is not self.X else self.distances[i][j], \n",
    "                np.partition(self.distances[j], self.k)[self.k])\n",
    "                for j in neighbors\n",
    "            ]) + 1e-10)\n",
    "            \n",
    "            ratios = [lrd_scores_train[j] / lrd_x for j in neighbors]\n",
    "            scores.append(np.mean(ratios))\n",
    "        \n",
    "        return np.array(scores)\n",
    "        \n",
    "    \n",
    "    def get_distance_matrix(self):\n",
    "        \"\"\"Retourne la matrice de distance originale\"\"\"\n",
    "        return self.distances\n",
    "    \n",
    "    def get_cluster_labels(self, X, threshold=None):\n",
    "        \"\"\"Return labels for input data X\"\"\"\n",
    "        scores = self.anomaly_score(X)\n",
    "        threshold = threshold or self.threshold\n",
    "        return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- One-Class SVM Simplifié -----\n",
    "class OneClassSVM_RBF:\n",
    "    def __init__(self, gamma=0.1):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_train = X\n",
    "        self.kernel = self._rbf_kernel(X, X)\n",
    "\n",
    "    def _rbf_kernel(self, X1, X2):\n",
    "        sq_dists = cdist(X1, X2, 'sqeuclidean')\n",
    "        return np.exp(-self.gamma * sq_dists)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        K = self._rbf_kernel(X, self.X_train)\n",
    "        return np.mean(K, axis=1)\n",
    "\n",
    "    def anomaly_score(self, X):\n",
    "        return -self.decision_function(X)\n",
    "\n",
    "#------Autoencoder------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z), z\n",
    "\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # === 1. Vérification et conversion des données ===\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values.astype(np.float32)\n",
    "        elif not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"Input must be DataFrame or numpy array\")\n",
    "        \n",
    "        # === 2. Initialisation du modèle ===\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "        # === 3. Entraînement ===\n",
    "        self.train()\n",
    "        for epoch in range(30):\n",
    "            epoch_loss = 0\n",
    "            for batch in loader:\n",
    "                x_batch = batch[0]\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, _ = self(x_batch)\n",
    "                loss = criterion(x_recon, x_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                avg_loss = epoch_loss / len(loader)\n",
    "                print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # === 4. Encodage dans l'espace latent ===\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            _, latent = self(X_tensor)\n",
    "            latent_np = latent.cpu().numpy()\n",
    "        \n",
    "        # === 5. Clustering ===\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        y_pred = kmeans.fit_predict(latent_np)\n",
    "        print(f\" Kmeans Silhouette Score: {silhouette_score(latent_np, y_pred):.3f}\")\n",
    "        # LOF\n",
    "        lof = LOF(k=20)\n",
    "        lof.fit(latent_np)\n",
    "        lof_scores = lof.anomaly_score()\n",
    "        lof_labels = lof.get_cluster_labels(pd.DataFrame(latent_np))\n",
    "        print(f\"LOF Silhouette Score: {silhouette_score(lof.get_distance_matrix(), lof_labels, metric='precomputed'):.3f}\")\n",
    "        \n",
    "        # Isolation Forest\n",
    "        iso_forest = IsolationForestScratch(n_trees=100)\n",
    "        iso_forest.fit(pd.DataFrame(latent_np))\n",
    "        iso_scores = iso_forest.anomaly_score(pd.DataFrame(latent_np))\n",
    "        iso_labels = iso_forest.get_cluster_labels(pd.DataFrame(latent_np))\n",
    "        print(f\"Isolation Forest Silhouette Score: {silhouette_score(iso_forest.get_distance_matrix(), iso_labels, metric='precomputed'):.3f}\")\n",
    "        self.latent_np = latent_np\n",
    "        self.lof = lof\n",
    "        self.iforest = iso_forest\n",
    "        self.lof_labels = lof_labels\n",
    "        self.iso_labels = iso_labels\n",
    "        return {\n",
    "            'latent': pd.DataFrame(latent_np),\n",
    "            'cluster_labels': y_pred,\n",
    "            'lof_scores': lof_scores,\n",
    "            'iso_scores': iso_scores,\n",
    "            'iso_labels': iso_labels,\n",
    "            'lof_labels': lof_labels\n",
    "        }\n",
    "    def encode(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values.astype(np.float32)\n",
    "        elif not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"Input must be DataFrame or numpy array\")\n",
    "\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(device)\n",
    "            latent = self.encoder(X_tensor)\n",
    "            return latent.cpu().numpy()\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values.astype(np.float32)\n",
    "\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(device)\n",
    "            X_recon = self.decoder(self.encoder(X_tensor))\n",
    "            return X_recon.cpu().numpy()\n",
    "\n",
    "#--------META MODELE------\n",
    "class BayesianOrdinalEncoder:\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        X_ = X.copy()\n",
    "        self.encoder = ce.LeaveOneOutEncoder(cols=self.cols, random_state=42, sigma=0.1)\n",
    "        return self.encoder.fit_transform(X_, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        return self.encoder.transform(X_)\n",
    "class ShrinkageBoostingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=5, stack_model=None, fast_mode=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = None\n",
    "        \n",
    "        self.fast_mode = fast_mode\n",
    "        \n",
    "        if stack_model is None:\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            self.stack_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        else:\n",
    "            self.stack_model = stack_model\n",
    "\n",
    "    def _preprocess(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            col_str = str(col)\n",
    "            if 'date' in col_str.lower():\n",
    "                try:\n",
    "                    X[col] = pd.to_datetime(X[col], errors='coerce').astype('int64') // 10**9\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                X[col] = X[col].fillna('missing')\n",
    "            else:\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "        cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "        if cat_cols:\n",
    "            self.encoder = BayesianOrdinalEncoder(cols=cat_cols)\n",
    "            X = self.encoder.fit_transform(X, y)\n",
    "\n",
    "        self.feature_names_ = X.columns.tolist()\n",
    "\n",
    "        X = pd.DataFrame(self.scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "        F = np.zeros(len(y))\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            p = 1 / (1 + np.exp(-F))\n",
    "            grad = y - p\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, grad)\n",
    "\n",
    "            pred = tree.predict(X)\n",
    "            delta = self.learning_rate * pred\n",
    "            F += delta\n",
    "\n",
    "            ll = log_loss(y, 1 / (1 + np.exp(-F)))\n",
    "            weight = 1 / (ll + 1e-5)\n",
    "            self.trees.append(tree)\n",
    "            self.tree_weights.append(weight)\n",
    "\n",
    "        stacked_preds = self._get_tree_outputs(X)\n",
    "        if self.stack_model is None:\n",
    "            from catboost import CatBoostClassifier\n",
    "            self.stack_model = CatBoostClassifier(\n",
    "                iterations=50,\n",
    "                learning_rate=0.1,\n",
    "                depth=4,\n",
    "                verbose=0,\n",
    "                random_seed=42,\n",
    "                task_type='GPU' if self.fast_mode else 'CPU'\n",
    "            )\n",
    "        # Fit le stack model une seule fois\n",
    "        self.stack_model.fit(stacked_preds, y)\n",
    "\n",
    "        # Calibration des probabilités : \"cv='prefit'\" car CatBoost est déjà entraîné\n",
    "        # self.stack_model = CalibratedClassifierCV(self.stack_model, method=\"sigmoid\", cv=\"prefit\")\n",
    "\n",
    "        # # Fit du calibrateur uniquement (pas le modèle CatBoost)\n",
    "        # self.stack_model.fit(stacked_preds, y)\n",
    "\n",
    "    def _get_tree_outputs(self, X):\n",
    "        outputs = [tree.predict(X) * w for tree, w in zip(self.trees, self.tree_weights)]\n",
    "        return np.vstack(outputs).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        X = self._preprocess(X)\n",
    "        if self.encoder:\n",
    "            X = self.encoder.transform(X)\n",
    "\n",
    "        X = pd.DataFrame(self.scaler.transform(X), columns=self.feature_names_)\n",
    "        stacked_preds = self._get_tree_outputs(X)\n",
    "        return self.stack_model.predict_proba(stacked_preds)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56244a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example Usage\\nfor name, features in feature_sets.items():\\n    try:\\n        print(f\"\\nRunning on {name}\")\\n        X = preprocess_data(df, features)\\n        labels, centers, sse = noise_based_kmeans(X)\\n        silhouette = silhouette_score(X, labels)\\n        print(f\"Silhouette Score: {silhouette:.4f}, SSE: {sse:.2f}, Clusters: {len(set(labels))}\")\\n        \\n        # Show cluster plot\\n        visualize_clusters(X, labels, title=f\"Clusters for {name}\")\\n\\n    except Exception as e:\\n        print(f\"Failed on {name}: {e}\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def preprocess_data(df, features):\n",
    "    df_subset = df[features].copy()\n",
    "\n",
    "    all_ordinal_cols = ['full_date', 'lib_jour', 'lib_mois', 'Birth_date', 'Activation_Date',\n",
    "                        'First_Call_Date', 'Last_Call_Date', 'status_date', 'Document_Validation_Date',\n",
    "                        'DOC_SCN_DT']\n",
    "    ordinal_cols = [col for col in all_ordinal_cols if col in df_subset.columns]\n",
    "    num_cols = df_subset.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat_cols = df_subset.select_dtypes(include='object').columns.tolist()\n",
    "    counter_cols = [col for col in cat_cols if col not in ordinal_cols]\n",
    "\n",
    "    # Pipeline numérique\n",
    "    num_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    df_num = pd.DataFrame(num_pipe.fit_transform(df_subset[num_cols]), columns=num_cols, index=df_subset.index).astype(np.float32)\n",
    "\n",
    "    # Pipeline ordinal\n",
    "    if ordinal_cols:\n",
    "        ordinal_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        df_ord = pd.DataFrame(ordinal_pipe.fit_transform(df_subset[ordinal_cols]), columns=ordinal_cols, index=df_subset.index).astype(np.float32)\n",
    "    else:\n",
    "        df_ord = pd.DataFrame(index=df_subset.index)\n",
    "\n",
    "    # Pipeline count encoding\n",
    "    if counter_cols:\n",
    "        count_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', ce.CountEncoder())\n",
    "        ])\n",
    "        df_count = pd.DataFrame(count_pipe.fit_transform(df_subset[counter_cols]), columns=counter_cols, index=df_subset.index)\n",
    "        df_count = np.log1p(df_count).astype(np.float32)\n",
    "    else:\n",
    "        df_count = pd.DataFrame(index=df_subset.index)\n",
    "\n",
    "    df_processed = pd.concat([df_num, df_ord, df_count], axis=1)\n",
    "\n",
    "    # Nettoyage final\n",
    "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_processed.fillna(0, inplace=True)\n",
    "\n",
    "    return df_processed.values\n",
    "\n",
    "\n",
    "def compute_sse(data, centers, labels):\n",
    "    return np.sum((data - centers[labels]) ** 2)\n",
    "\n",
    "def noise_based_kmeans(X, max_iter=50, rmin=0.1, rmax=0.5, K_range=(2, 10)):\n",
    "    N, D = X.shape\n",
    "    if N < 2:\n",
    "        raise ValueError(\"Not enough data points for clustering.\")\n",
    "\n",
    "    best_sse = float('inf')\n",
    "    best_labels = None\n",
    "    best_centers = None\n",
    "\n",
    "    NS = max(1, int(np.sqrt(N) * max_iter))  # Avoid zero\n",
    "    sqrtN = max(1, int(np.sqrt(N)))\n",
    "    restart = max(1, max_iter // sqrtN)\n",
    "    decrease = (rmax - rmin) / max(1, (max_iter - 1))\n",
    "    noise_radius = rmax\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        K = random.randint(*K_range)\n",
    "        if K >= N:\n",
    "            K = N - 1  # Avoid having more clusters than data points\n",
    "\n",
    "        init_indices = np.random.choice(range(N), K, replace=False)\n",
    "        centers = X[init_indices]\n",
    "\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_radius, size=centers.shape)\n",
    "        centers += noise\n",
    "\n",
    "        distances = euclidean_distances(X, centers)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        new_centers = []\n",
    "        for i in range(K):\n",
    "            cluster_points = X[labels == i]\n",
    "            if len(cluster_points) == 0:\n",
    "                # Fallback: reinitialize center if cluster is empty\n",
    "                new_centers.append(X[random.randint(0, N - 1)])\n",
    "            else:\n",
    "                new_centers.append(cluster_points.mean(axis=0))\n",
    "        new_centers = np.array(new_centers)\n",
    "\n",
    "        sse = compute_sse(X, new_centers, labels)\n",
    "\n",
    "        if sse < best_sse:\n",
    "            best_sse = sse\n",
    "            best_labels = labels\n",
    "            best_centers = new_centers\n",
    "\n",
    "        noise_radius = max(noise_radius - decrease, rmin)\n",
    "        if iteration % restart == 0:\n",
    "            noise_radius = rmax\n",
    "\n",
    "    return best_labels, best_centers, best_sse\n",
    "\n",
    "\n",
    "\n",
    "def visualize_clusters(X, labels, title=\"Cluster Visualization\"):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "'''# Example Usage\n",
    "for name, features in feature_sets.items():\n",
    "    try:\n",
    "        print(f\"\\nRunning on {name}\")\n",
    "        X = preprocess_data(df, features)\n",
    "        labels, centers, sse = noise_based_kmeans(X)\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        print(f\"Silhouette Score: {silhouette:.4f}, SSE: {sse:.2f}, Clusters: {len(set(labels))}\")\n",
    "        \n",
    "        # Show cluster plot\n",
    "        visualize_clusters(X, labels, title=f\"Clusters for {name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {name}: {e}\")'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "455907e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "class FeatureSelectorForAnomaly:\n",
    "    def __init__(self, n_features=40, corr_threshold=0.85):\n",
    "        self.n_features = n_features\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.selected_features_stage1 = []\n",
    "        self.selected_features_final = []\n",
    "        self.preprocessor = None\n",
    "        self.all_features = []\n",
    "\n",
    "    def _prepare_data(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        all_ordinal_cols = ['full_date', 'lib_jour', 'lib_mois', 'Birth_date', 'Activation_Date',\n",
    "                            'First_Call_Date', 'Last_Call_Date', 'status_date', 'Document_Validation_Date',\n",
    "                            'DOC_SCN_DT']\n",
    "        ordinal_cols = [col for col in all_ordinal_cols if col in df.columns]\n",
    "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        cat_cols = df.select_dtypes(include=['object', 'datetime64[ns]']).columns.tolist()\n",
    "        counter_cols = [col for col in cat_cols if col not in ordinal_cols]\n",
    "\n",
    "        # Pipeline numérique\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        df_num = pd.DataFrame(num_pipe.fit_transform(df[num_cols]), columns=num_cols, index=df.index).astype(np.float32)\n",
    "\n",
    "        # Pipeline ordinal\n",
    "        if ordinal_cols:\n",
    "            ordinal_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ])\n",
    "            df_ord = pd.DataFrame(ordinal_pipe.fit_transform(df[ordinal_cols]), columns=ordinal_cols, index=df.index).astype(np.float32)\n",
    "        else:\n",
    "            df_ord = pd.DataFrame(index=df.index)\n",
    "\n",
    "        # Pipeline count encoding\n",
    "        if counter_cols:\n",
    "            count_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', ce.CountEncoder())\n",
    "            ])\n",
    "            df_count = pd.DataFrame(count_pipe.fit_transform(df[counter_cols]), columns=counter_cols, index=df.index)\n",
    "            df_count = np.log1p(df_count).astype(np.float32)\n",
    "        else:\n",
    "            df_count = pd.DataFrame(index=df.index)\n",
    "\n",
    "        df_processed = pd.concat([df_num, df_ord, df_count], axis=1)\n",
    "\n",
    "        # Nettoyage final\n",
    "        df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df_processed.fillna(0, inplace=True)\n",
    "\n",
    "        self.all_features = df_processed.columns.tolist()\n",
    "        return df_processed.values, self.all_features\n",
    "\n",
    "\n",
    "    def _remove_correlated(self, X_df, features, threshold=0.85):\n",
    "        corr_matrix = X_df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "        return [feat for feat in features if feat not in to_drop]\n",
    "\n",
    "    \n",
    "    def _unsupervised_feature_scores(self, X, features):\n",
    "        scores = {}\n",
    "\n",
    "        # 1. Variance\n",
    "        variances = np.var(X, axis=0)\n",
    "        scores['variance'] = variances / variances.max()\n",
    "\n",
    "        # 2. Isolation Forest scratch\n",
    "        iso = IsolationForestScratch(n_trees=100)\n",
    "        iso.fit(X)\n",
    "        scores['isoforest'] = np.abs(iso.anomaly_score(X)).mean(axis=0)\n",
    "\n",
    "        # 3. LOF + PCA\n",
    "        pca = PCA(n_components=min(10, X.shape[1]))\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        lof = LOF(k=20)\n",
    "        lof.fit(X_pca)\n",
    "        lof_scores = lof.anomaly_score()\n",
    "\n",
    "        # Seuil basé sur le 90e percentile\n",
    "        labels = (lof_scores > np.percentile(lof_scores, 90)).astype(int)\n",
    "\n",
    "        scores['pca_info'] = pd.Series(\n",
    "            mutual_info_classif(X, labels, discrete_features=False),\n",
    "            index=features\n",
    "        )\n",
    "\n",
    "        all_scores = pd.DataFrame(scores, index=features)\n",
    "        final_score = all_scores.mean(axis=1)\n",
    "        return final_score.sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "    def select_top_features(self, df):\n",
    "        print(\"[Étape 1] Préparation des données...\")\n",
    "        X, features = self._prepare_data(df)\n",
    "        X_df = pd.DataFrame(X, columns=features)\n",
    "\n",
    "        print(\"[Étape 2] Calcul des scores de features non supervisés...\")\n",
    "        score_series = self._unsupervised_feature_scores(X, features)\n",
    "\n",
    "        print(\"[Étape 3] Sélection des\", self.n_features, \"meilleures features brutes...\")\n",
    "        top_feats = score_series.head(self.n_features).index.tolist()\n",
    "\n",
    "        print(\"[Étape 4] Suppression des features corrélées (>\", self.corr_threshold, \")...\")\n",
    "        reduced_feats = self._remove_correlated(X_df[top_feats], top_feats, threshold=self.corr_threshold)\n",
    "        self.selected_features_stage1 = reduced_feats[:self.n_features]\n",
    "        print(\"[Résultat] Features sélectionnées (étape 1) :\", len(self.selected_features_stage1))\n",
    "        return self.selected_features_stage1\n",
    "        \n",
    "\n",
    "    def _evaluate_who(self, X, individual):\n",
    "        selected = np.where(individual == 1)[0]\n",
    "        if len(selected) < 2:\n",
    "            return np.inf  # Penalize small subsets\n",
    "\n",
    "        subset = X[:, selected]\n",
    "        try:\n",
    "            # 1. IsolationForest-based clustering\n",
    "            iso_labels = IsolationForest(random_state=0).fit_predict(subset)\n",
    "            if len(np.unique(iso_labels)) < 2:\n",
    "                return np.inf  # Penalize single-cluster solutions\n",
    "            sil_iso = silhouette_score(subset, iso_labels)\n",
    "\n",
    "            # 2. Noise-based KMeans clustering\n",
    "            nbk_labels, _, _ = noise_based_kmeans(subset)\n",
    "            if len(np.unique(nbk_labels)) < 2:\n",
    "                return np.inf\n",
    "            sil_nbk = silhouette_score(subset, nbk_labels)\n",
    "\n",
    "            # Combine the two silhouette scores\n",
    "            avg_sil = (sil_iso + sil_nbk) / 2.0\n",
    "\n",
    "            return -avg_sil + 0.001 * len(selected)  # Minimize loss = -score + complexity_penalty\n",
    "\n",
    "        except Exception as e:\n",
    "            return np.inf\n",
    "\n",
    "\n",
    "    def optimize_with_who(self, X, n_horses=40, n_generations=20, n_stallions=5, mutation_rate=0.1):\n",
    "        print(\"[WHO] Optimisation via Wild Horse Optimizer\")\n",
    "        feature_indices = [self.all_features.index(f) for f in self.selected_features_stage1]\n",
    "        n_features = len(feature_indices)\n",
    "        # Convert to numpy array first if X is a DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_sel = X.iloc[:, feature_indices].values.astype(np.float32)\n",
    "        else:\n",
    "            X_sel = X[:, feature_indices].astype(np.float32)\n",
    "\n",
    "        # 1. Initialize population (horses)\n",
    "        population = np.random.binomial(1, 0.5, size=(n_horses, n_features))\n",
    "        \n",
    "        # 2. Evaluate initial population\n",
    "        fitness = np.array([self._evaluate_who(X_sel, ind) for ind in population])\n",
    "        best_idx = np.argmin(fitness)\n",
    "        best_solution = population[best_idx].copy()\n",
    "        best_score = fitness[best_idx]\n",
    "\n",
    "        for gen in tqdm(range(n_generations), desc=\"WHO Generations\"):\n",
    "            # 3. Select stallions (best solutions)\n",
    "            stallion_indices = np.argsort(fitness)[:n_stallions]\n",
    "            stallions = population[stallion_indices]\n",
    "\n",
    "            # 4. Group into harems (assign each horse to a stallion)\n",
    "            harem_sizes = np.random.randint(1, n_horses - n_stallions + 1, size=n_stallions)\n",
    "            harem_sizes = (harem_sizes / harem_sizes.sum() * (n_horses - n_stallions)).astype(int)\n",
    "            harem_sizes[-1] = n_horses - n_stallions - harem_sizes[:-1].sum()  # Adjust last group\n",
    "\n",
    "            # 5. Grazing behavior (exploration)\n",
    "            new_population = []\n",
    "            for i, stallion in enumerate(stallions):\n",
    "                harem_size = harem_sizes[i]\n",
    "                harem = np.random.binomial(1, 0.5, size=(harem_size, n_features))  # Random exploration\n",
    "                harem = np.where(np.random.rand(harem_size, n_features) < 0.3, stallion, harem)  # Follow stallion\n",
    "                new_population.append(harem)\n",
    "            \n",
    "            new_population = np.vstack([stallions] + new_population)\n",
    "            \n",
    "            # 6. Breeding (crossover between stallions)\n",
    "            for i in range(1, n_stallions):\n",
    "                crossover_point = np.random.randint(1, n_features)\n",
    "                new_population[i, :crossover_point] = new_population[0, :crossover_point]  # Alpha stallion influence\n",
    "\n",
    "            # 7. Mutation\n",
    "            mutation_mask = np.random.rand(n_horses, n_features) < mutation_rate\n",
    "            new_population = np.where(mutation_mask, 1 - new_population, new_population)\n",
    "\n",
    "            # 8. Update population and fitness\n",
    "            population = new_population\n",
    "            fitness = np.array([self._evaluate_who(X_sel, ind) for ind in population])\n",
    "            \n",
    "            # 9. Track best solution\n",
    "            current_best_idx = np.argmin(fitness)\n",
    "            if fitness[current_best_idx] < best_score:\n",
    "                best_solution = population[current_best_idx].copy()\n",
    "                best_score = fitness[current_best_idx]\n",
    "            print(f\"[WHO] {len(self.selected_features_final)} features selected | Best score: {best_score:.4f}\")\n",
    "        self.selected_features_final = [self.selected_features_stage1[i] for i in range(n_features) if best_solution[i] == 1]\n",
    "        print(f\"[WHO] {len(self.selected_features_final)} features selected | Best score: {best_score:.4f}\")\n",
    "        return self.selected_features_final\n",
    "\n",
    "    \n",
    "    def transform_final(self, df):\n",
    "        # Use the same preprocessing as in _prepare_data\n",
    "        X_processed, _ = self._prepare_data(df)\n",
    "        \n",
    "        # Get indices of selected features\n",
    "        feat_idx = [self.all_features.index(f) for f in self.selected_features_final]\n",
    "        \n",
    "        # Return only the selected columns\n",
    "        return X_processed[:, feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea163fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Étape 1] Préparation des données...\n",
      "[Étape 2] Calcul des scores de features non supervisés...\n",
      "[Étape 3] Sélection des 40 meilleures features brutes...\n",
      "[Étape 4] Suppression des features corrélées (> 0.85 )...\n",
      "[Résultat] Features sélectionnées (étape 1) : 36\n",
      "Top features sélectionnées sans corrélation : ['Last_Call_Date', 'First_Call_Date', 'reactivite_client', 'full_date', 'Activation_Date', 'temps_moyen_appel', 'cust_id', 'pdv_sk', 'id_date', 'DOC_VAL_USR', 'localisation_sk', 'temps_moyen_traitement', 'ARPU_Last_2_Months', 'Revenue_Last_2_Months', 'Document_Validation_3G', 'Document_Validation_2G', 'lib_jour', 'Id_operation', 'PoS_ID', 'ICC', 'Customer_Type', 'Source', 'lib_mois', 'nin', 'Document_type', 'MSISDN', 'minor_ok', 'doc_scan_avant_activation', 'Street', 'age_sub', 'similarity_score_bin', 'Province', 'City', 'ID_junk', 'DOC_SCN_USR', 'Document_Validation_4G']\n",
      "[WHO] Optimisation via Wild Horse Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:   5%|▌         | 1/20 [00:56<17:47, 56.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  10%|█         | 2/20 [01:52<16:49, 56.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  15%|█▌        | 3/20 [02:51<16:14, 57.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  20%|██        | 4/20 [03:49<15:22, 57.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  25%|██▌       | 5/20 [04:47<14:26, 57.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  30%|███       | 6/20 [05:42<13:17, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  35%|███▌      | 7/20 [06:32<11:52, 54.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  40%|████      | 8/20 [07:29<11:05, 55.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  45%|████▌     | 9/20 [08:27<10:17, 56.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  50%|█████     | 10/20 [09:54<10:57, 65.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  55%|█████▌    | 11/20 [11:00<09:53, 65.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  60%|██████    | 12/20 [11:57<08:24, 63.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9910\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "selector = FeatureSelectorForAnomaly(n_features=40)\n",
    "selected_stage1 = selector.select_top_features(df)\n",
    "\n",
    "print(\"Top features sélectionnées sans corrélation :\", selected_stage1)\n",
    "\n",
    "# WHO sur les 40\n",
    "X_all, features = selector._prepare_data(df)\n",
    "selected_final = selector.optimize_with_who(X_all)\n",
    "\n",
    "\n",
    "print(\"Features finales sélectionnées par WHO :\", selected_final)\n",
    "\n",
    "X_final = selector.transform_final(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209686f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "class FeatureSelectorForAnomaly:\n",
    "    def __init__(self, n_features=40):\n",
    "        self.n_features = n_features\n",
    "        self.selected_features_stage1 = []\n",
    "        self.selected_features_final = []\n",
    "        self.preprocessor = None\n",
    "        self.all_features = []\n",
    "\n",
    "    def _prepare_data(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        all_ordinal_cols = ['full_date', 'lib_jour', 'lib_mois', 'Birth_date', 'Activation_Date',\n",
    "                            'First_Call_Date', 'Last_Call_Date', 'status_date', 'Document_Validation_Date',\n",
    "                            'DOC_SCN_DT']\n",
    "        ordinal_cols = [col for col in all_ordinal_cols if col in df.columns]\n",
    "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "        counter_cols = [col for col in cat_cols if col not in ordinal_cols]\n",
    "\n",
    "        # Pipeline numérique\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        df_num = pd.DataFrame(num_pipe.fit_transform(df[num_cols]), columns=num_cols, index=df.index).astype(np.float32)\n",
    "\n",
    "        # Pipeline ordinal\n",
    "        if ordinal_cols:\n",
    "            ordinal_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ])\n",
    "            df_ord = pd.DataFrame(ordinal_pipe.fit_transform(df[ordinal_cols]), columns=ordinal_cols, index=df.index).astype(np.float32)\n",
    "        else:\n",
    "            df_ord = pd.DataFrame(index=df.index)\n",
    "\n",
    "        # Pipeline count encoding\n",
    "        if counter_cols:\n",
    "            count_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', ce.CountEncoder())\n",
    "            ])\n",
    "            df_count = pd.DataFrame(count_pipe.fit_transform(df[counter_cols]), columns=counter_cols, index=df.index)\n",
    "            df_count = np.log1p(df_count).astype(np.float32)\n",
    "        else:\n",
    "            df_count = pd.DataFrame(index=df.index)\n",
    "\n",
    "        df_processed = pd.concat([df_num, df_ord, df_count], axis=1)\n",
    "\n",
    "        # Nettoyage final\n",
    "        df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df_processed.fillna(0, inplace=True)\n",
    "\n",
    "        self.all_features = df_processed.columns.tolist()\n",
    "        return df_processed.values, self.all_features\n",
    "\n",
    "\n",
    "    # def _remove_correlated(self, X_df, features, threshold=0.85):\n",
    "    #     corr_matrix = X_df.corr().abs()\n",
    "    #     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    #     to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    #     return [feat for feat in features if feat not in to_drop]\n",
    "\n",
    "    \n",
    "    def _unsupervised_feature_scores(self, X, features):\n",
    "        scores = {}\n",
    "\n",
    "        # 1. Variance\n",
    "        variances = np.var(X, axis=0)\n",
    "        scores['variance'] = variances / variances.max()\n",
    "\n",
    "        # 2. Isolation Forest scratch\n",
    "        iso = IsolationForestScratch(n_trees=100)\n",
    "        iso.fit(X)\n",
    "        scores['isoforest'] = np.abs(iso.anomaly_score(X)).mean(axis=0)\n",
    "\n",
    "        # 3. LOF + PCA\n",
    "        pca = PCA(n_components=min(10, X.shape[1]))\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        lof = LOF(k=20)\n",
    "        lof.fit(X_pca)\n",
    "        lof_scores = lof.anomaly_score()\n",
    "\n",
    "        # Seuil basé sur le 90e percentile\n",
    "        labels = (lof_scores > np.percentile(lof_scores, 90)).astype(int)\n",
    "\n",
    "        scores['pca_info'] = pd.Series(\n",
    "            mutual_info_classif(X, labels, discrete_features=False),\n",
    "            index=features\n",
    "        )\n",
    "\n",
    "        all_scores = pd.DataFrame(scores, index=features)\n",
    "        final_score = all_scores.mean(axis=1)\n",
    "        return final_score.sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "    def select_top_features(self, df):\n",
    "        print(\"[Étape 1] Préparation des données...\")\n",
    "        X, features = self._prepare_data(df)\n",
    "        X_df = pd.DataFrame(X, columns=features)\n",
    "\n",
    "        print(\"[Étape 2] Calcul des scores de features non supervisés...\")\n",
    "        score_series = self._unsupervised_feature_scores(X, features)\n",
    "\n",
    "        print(\"[Étape 3] Sélection des\", self.n_features, \"meilleures features brutes...\")\n",
    "        top_feats = score_series.head(self.n_features).index.tolist()\n",
    "\n",
    "        # print(\"[Étape 4] Suppression des features corrélées (>\", self.corr_threshold, \")...\")\n",
    "        # reduced_feats = self._remove_correlated(X_df[top_feats], top_feats, threshold=self.corr_threshold)\n",
    "        self.selected_features_stage1 = top_feats[:self.n_features]\n",
    "        print(\"[Résultat] Features sélectionnées (étape 1) :\", len(self.selected_features_stage1))\n",
    "        return self.selected_features_stage1\n",
    "        \n",
    "\n",
    "    def _evaluate_who(self, X, individual):\n",
    "        selected = np.where(individual == 1)[0]\n",
    "        if len(selected) < 2:\n",
    "            return np.inf  # Penalize small subsets\n",
    "\n",
    "        subset = X[:, selected]\n",
    "        try:\n",
    "            # 1. IsolationForest-based clustering\n",
    "            iso_labels = IsolationForest(random_state=0).fit_predict(subset)\n",
    "            if len(np.unique(iso_labels)) < 2:\n",
    "                return np.inf  # Penalize single-cluster solutions\n",
    "            sil_iso = silhouette_score(subset, iso_labels)\n",
    "\n",
    "            # 2. Noise-based KMeans clustering\n",
    "            nbk_labels, _, _ = noise_based_kmeans(subset)\n",
    "            if len(np.unique(nbk_labels)) < 2:\n",
    "                return np.inf\n",
    "            sil_nbk = silhouette_score(subset, nbk_labels)\n",
    "\n",
    "            # Combine the two silhouette scores\n",
    "            avg_sil = (sil_iso + sil_nbk) / 2.0\n",
    "\n",
    "            return -avg_sil + 0.001 * len(selected)  # Minimize loss = -score + complexity_penalty\n",
    "\n",
    "        except Exception as e:\n",
    "            return np.inf\n",
    "\n",
    "\n",
    "    def optimize_with_who(self, X, n_horses=40, n_generations=20, n_stallions=5, mutation_rate=0.1):\n",
    "        print(\"[WHO] Optimisation via Wild Horse Optimizer\")\n",
    "        feature_indices = [self.all_features.index(f) for f in self.selected_features_stage1]\n",
    "        n_features = len(feature_indices)\n",
    "        # Convert to numpy array first if X is a DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_sel = X.iloc[:, feature_indices].values.astype(np.float32)\n",
    "        else:\n",
    "            X_sel = X[:, feature_indices].astype(np.float32)\n",
    "\n",
    "        # 1. Initialize population (horses)\n",
    "        population = np.random.binomial(1, 0.5, size=(n_horses, n_features))\n",
    "        \n",
    "        # 2. Evaluate initial population\n",
    "        fitness = np.array([self._evaluate_who(X_sel, ind) for ind in population])\n",
    "        best_idx = np.argmin(fitness)\n",
    "        best_solution = population[best_idx].copy()\n",
    "        best_score = fitness[best_idx]\n",
    "\n",
    "        for gen in tqdm(range(n_generations), desc=\"WHO Generations\"):\n",
    "            # 3. Select stallions (best solutions)\n",
    "            stallion_indices = np.argsort(fitness)[:n_stallions]\n",
    "            stallions = population[stallion_indices]\n",
    "\n",
    "            # 4. Group into harems (assign each horse to a stallion)\n",
    "            harem_sizes = np.random.randint(1, n_horses - n_stallions + 1, size=n_stallions)\n",
    "            harem_sizes = (harem_sizes / harem_sizes.sum() * (n_horses - n_stallions)).astype(int)\n",
    "            harem_sizes[-1] = n_horses - n_stallions - harem_sizes[:-1].sum()  # Adjust last group\n",
    "\n",
    "            # 5. Grazing behavior (exploration)\n",
    "            new_population = []\n",
    "            for i, stallion in enumerate(stallions):\n",
    "                harem_size = harem_sizes[i]\n",
    "                harem = np.random.binomial(1, 0.5, size=(harem_size, n_features))  # Random exploration\n",
    "                harem = np.where(np.random.rand(harem_size, n_features) < 0.3, stallion, harem)  # Follow stallion\n",
    "                new_population.append(harem)\n",
    "            \n",
    "            new_population = np.vstack([stallions] + new_population)\n",
    "            \n",
    "            # 6. Breeding (crossover between stallions)\n",
    "            for i in range(1, n_stallions):\n",
    "                crossover_point = np.random.randint(1, n_features)\n",
    "                new_population[i, :crossover_point] = new_population[0, :crossover_point]  # Alpha stallion influence\n",
    "\n",
    "            # 7. Mutation\n",
    "            mutation_mask = np.random.rand(n_horses, n_features) < mutation_rate\n",
    "            new_population = np.where(mutation_mask, 1 - new_population, new_population)\n",
    "\n",
    "            # 8. Update population and fitness\n",
    "            population = new_population\n",
    "            fitness = np.array([self._evaluate_who(X_sel, ind) for ind in population])\n",
    "            \n",
    "            # 9. Track best solution\n",
    "            current_best_idx = np.argmin(fitness)\n",
    "            if fitness[current_best_idx] < best_score:\n",
    "                best_solution = population[current_best_idx].copy()\n",
    "                best_score = fitness[current_best_idx]\n",
    "            print(f\"[WHO] {len(self.selected_features_final)} features selected | Best score: {best_score:.4f}\")\n",
    "        self.selected_features_final = [self.selected_features_stage1[i] for i in range(n_features) if best_solution[i] == 1]\n",
    "        print(f\"[WHO] {len(self.selected_features_final)} features selected | Best score: {best_score:.4f}\")\n",
    "        return self.selected_features_final, best_score\n",
    "\n",
    "    \n",
    "    def transform_final(self, df):\n",
    "        # Use the same preprocessing as in _prepare_data\n",
    "        X_processed, _ = self._prepare_data(df)\n",
    "        \n",
    "        # Get indices of selected features\n",
    "        feat_idx = [self.all_features.index(f) for f in self.selected_features_final]\n",
    "        \n",
    "        # Return only the selected columns\n",
    "        return X_processed[:, feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216c543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Étape 1] Préparation des données...\n",
      "[Étape 2] Calcul des scores de features non supervisés...\n",
      "[Étape 3] Sélection des 40 meilleures features brutes...\n",
      "[Résultat] Features sélectionnées (étape 1) : 40\n",
      "Top features sélectionnées sans corrélation : ['Last_Call_Date', 'First_Call_Date', 'reactivite_client', 'Document_Validation_Date', 'DOC_SCN_DT', 'full_date', 'Activation_Date', 'temps_moyen_appel', 'cust_id', 'pdv_sk', 'id_date', 'lib_jour', 'Document_type', 'Document_Validation_2G', 'jours', 'rejection_reason', 'mois', 'NIN_ok', 'Street', 'taux_rejet', 'temps_moyen_traitement', 'ID_junk', 'localisation_sk', 'SIM_Type', 'nb_total_contrat_traiter', 'Connection_Type', 'Gender', 'taux_validite', 'MSISDN', 'Termination_Date', 'Province', 'nin', 'Status_Date', 'Revenue_Last_2_Months', 'Status', 'ARPU_Last_2_Months', 'Document_Validation_3G', 'Document_Stamped', 'type_d_opeartion', 'PoS_ID']\n",
      "[WHO] Optimisation via Wild Horse Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:   5%|▌         | 1/20 [01:34<29:59, 94.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  10%|█         | 2/20 [03:02<27:13, 90.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  15%|█▌        | 3/20 [04:32<25:31, 90.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  20%|██        | 4/20 [06:04<24:14, 90.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  25%|██▌       | 5/20 [07:34<22:38, 90.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  30%|███       | 6/20 [09:03<21:01, 90.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.7910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  35%|███▌      | 7/20 [10:33<19:31, 90.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  40%|████      | 8/20 [12:03<18:00, 90.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  45%|████▌     | 9/20 [13:35<16:36, 90.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  50%|█████     | 10/20 [15:06<15:07, 90.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  55%|█████▌    | 11/20 [16:29<13:15, 88.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  60%|██████    | 12/20 [17:23<10:23, 77.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  65%|██████▌   | 13/20 [18:18<08:18, 71.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  70%|███████   | 14/20 [19:13<06:37, 66.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  75%|███████▌  | 15/20 [20:12<05:20, 64.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  80%|████████  | 16/20 [21:11<04:10, 62.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  85%|████████▌ | 17/20 [22:08<03:02, 60.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  90%|█████████ | 18/20 [23:05<01:59, 59.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations:  95%|█████████▌| 19/20 [24:02<00:58, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WHO Generations: 100%|██████████| 20/20 [25:41<00:00, 77.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHO] 0 features selected | Best score: -0.9433\n",
      "[WHO] 14 features selected | Best score: -0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# WHO sur les 40\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_all, features \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39m_prepare_data(df)\n\u001b[1;32m----> 9\u001b[0m selected_final, score, K \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39moptimize_with_who(X_all)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures finales sélectionnées par WHO :\u001b[39m\u001b[38;5;124m\"\u001b[39m, selected_final)\n\u001b[0;32m     14\u001b[0m X_final \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform_final(df)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "selector = FeatureSelectorForAnomaly(n_features=40)\n",
    "selected_stage1 = selector.select_top_features(df)\n",
    "\n",
    "print(\"Top features sélectionnées sans corrélation :\", selected_stage1)\n",
    "\n",
    "# WHO sur les 40\n",
    "X_all, features = selector._prepare_data(df)\n",
    "selected_final = selector.optimize_with_who(X_all)\n",
    "\n",
    "\n",
    "print(\"Features finales sélectionnées par WHO :\", selected_final)\n",
    "\n",
    "X_final = selector.transform_final(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17577fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Lancement de 162 combinaisons avec joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 80.8min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 135.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 226.8min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 338.3min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 396.7min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 447.6min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 592.3min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 694.2min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 767.9min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 849.0min\n",
      "[Parallel(n_jobs=-1)]: Done 156 out of 162 | elapsed: 971.2min remaining: 37.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Résultats enregistrés dans 'feature_selector_experiments_joblib.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 1004.9min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "def run_experiment(params):\n",
    "    try:\n",
    "        n_feat, gen, mut_rate, n_horses, n_stallions = params\n",
    "        print(f\"▶️ Running: n_features={n_feat}, n_horses={n_horses}, generations={gen}, n_stallions={n_stallions}, mutation_rate={mut_rate}\")\n",
    "        \n",
    "        # Recharge les données dans chaque worker (évite les problèmes de mémoire partagée)\n",
    "        # df = pd.read_csv(\"your_data.csv\")  # Assurez-vous que le chemin est correct\n",
    "        \n",
    "        selector = FeatureSelectorForAnomaly(n_features=n_feat)\n",
    "        stage1_feats = selector.select_top_features(df)\n",
    "        X_all, features = selector._prepare_data(df)\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        final_feats, score = selector.optimize_with_who(\n",
    "            X_all, n_horses=n_horses, n_generations=gen,\n",
    "            n_stallions=n_stallions, mutation_rate=mut_rate\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'n_features': n_feat,\n",
    "            'n_horses': n_horses,\n",
    "            'generations': gen,\n",
    "            'n_stallions': n_stallions,\n",
    "            'mutation_rate': mut_rate,\n",
    "            'stage1_selected': len(stage1_feats),\n",
    "            'final_selected': len(final_feats),\n",
    "            'selected_features': list(final_feats),\n",
    "            'score': float(score),\n",
    "            'duration_sec': round(duration, 2)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[❌ Erreur] {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'n_features': n_feat,\n",
    "            'n_horses': n_horses,\n",
    "            'generations': gen,\n",
    "            'n_stallions': n_stallions,\n",
    "            'mutation_rate': mut_rate,\n",
    "            'stage1_selected': 0,\n",
    "            'final_selected': 0,\n",
    "            'selected_features': [],\n",
    "            'score': 'error',\n",
    "            'duration_sec': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_features_list = [20, 30, 40]\n",
    "    n_generations_list = [10, 20, 30]\n",
    "    mutation_rates = [0.05, 0.1]\n",
    "    n_horses_list = [20, 30, 40]\n",
    "    n_stallions_list = [3, 5, 7]\n",
    "\n",
    "    param_combinations = list(itertools.product(\n",
    "        n_features_list, n_generations_list, mutation_rates,\n",
    "        n_horses_list, n_stallions_list\n",
    "    ))\n",
    "\n",
    "    print(f\"🧠 Lancement de {len(param_combinations)} combinaisons avec joblib...\")\n",
    "\n",
    "    # Utilisation de joblib.Parallel\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(  # n_jobs=-1 utilise tous les cœurs\n",
    "        delayed(run_experiment)(params) for params in param_combinations\n",
    "    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('feature_selector_experiments_joblib.csv', index=False)\n",
    "    print(\"✅ Résultats enregistrés dans 'feature_selector_experiments_joblib.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e9dec",
   "metadata": {},
   "source": [
    "PSO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b77c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "class FeatureSelectorForAnomaly:\n",
    "    def __init__(self, n_features=40):\n",
    "        self.n_features = n_features\n",
    "        self.selected_features_stage1 = []\n",
    "        self.selected_features_final = []\n",
    "        self.preprocessor = None\n",
    "        self.all_features = []\n",
    "\n",
    "    def _prepare_data(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        all_ordinal_cols = ['full_date', 'lib_jour', 'lib_mois', 'Birth_date', 'Activation_Date',\n",
    "                            'First_Call_Date', 'Last_Call_Date', 'status_date', 'Document_Validation_Date',\n",
    "                            'DOC_SCN_DT']\n",
    "        ordinal_cols = [col for col in all_ordinal_cols if col in df.columns]\n",
    "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "        counter_cols = [col for col in cat_cols if col not in ordinal_cols]\n",
    "\n",
    "        # Pipeline numérique\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        df_num = pd.DataFrame(num_pipe.fit_transform(df[num_cols]), columns=num_cols, index=df.index).astype(np.float32)\n",
    "\n",
    "        # Pipeline ordinal\n",
    "        if ordinal_cols:\n",
    "            ordinal_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ])\n",
    "            df_ord = pd.DataFrame(ordinal_pipe.fit_transform(df[ordinal_cols]), columns=ordinal_cols, index=df.index).astype(np.float32)\n",
    "        else:\n",
    "            df_ord = pd.DataFrame(index=df.index)\n",
    "\n",
    "        # Pipeline count encoding\n",
    "        if counter_cols:\n",
    "            count_pipe = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', ce.CountEncoder())\n",
    "            ])\n",
    "            df_count = pd.DataFrame(count_pipe.fit_transform(df[counter_cols]), columns=counter_cols, index=df.index)\n",
    "            df_count = np.log1p(df_count).astype(np.float32)\n",
    "        else:\n",
    "            df_count = pd.DataFrame(index=df.index)\n",
    "\n",
    "        df_processed = pd.concat([df_num, df_ord, df_count], axis=1)\n",
    "\n",
    "        # Nettoyage final\n",
    "        df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df_processed.fillna(0, inplace=True)\n",
    "\n",
    "        self.all_features = df_processed.columns.tolist()\n",
    "        return df_processed.values, self.all_features\n",
    "\n",
    "\n",
    "    # def _remove_correlated(self, X_df, features, threshold=0.85):\n",
    "    #     corr_matrix = X_df.corr().abs()\n",
    "    #     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    #     to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    #     return [feat for feat in features if feat not in to_drop]\n",
    "\n",
    "    \n",
    "    def _unsupervised_feature_scores(self, X, features):\n",
    "        scores = {}\n",
    "\n",
    "        # 1. Variance\n",
    "        variances = np.var(X, axis=0)\n",
    "        scores['variance'] = variances / variances.max()\n",
    "\n",
    "        # 2. Isolation Forest scratch\n",
    "        iso = IsolationForestScratch(n_trees=100)\n",
    "        iso.fit(X)\n",
    "        scores['isoforest'] = np.abs(iso.anomaly_score(X)).mean(axis=0)\n",
    "\n",
    "        # 3. LOF + PCA\n",
    "        pca = PCA(n_components=min(10, X.shape[1]))\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        lof = LOF(k=20)\n",
    "        lof.fit(X_pca)\n",
    "        lof_scores = lof.anomaly_score()\n",
    "\n",
    "        # Seuil basé sur le 90e percentile\n",
    "        labels = (lof_scores > np.percentile(lof_scores, 90)).astype(int)\n",
    "\n",
    "        scores['pca_info'] = pd.Series(\n",
    "            mutual_info_classif(X, labels, discrete_features=False),\n",
    "            index=features\n",
    "        )\n",
    "\n",
    "        all_scores = pd.DataFrame(scores, index=features)\n",
    "        final_score = all_scores.mean(axis=1)\n",
    "        return final_score.sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "    def select_top_features(self, df):\n",
    "        print(\"[Étape 1] Préparation des données...\")\n",
    "        X, features = self._prepare_data(df)\n",
    "        X_df = pd.DataFrame(X, columns=features)\n",
    "\n",
    "        print(\"[Étape 2] Calcul des scores de features non supervisés...\")\n",
    "        score_series = self._unsupervised_feature_scores(X, features)\n",
    "\n",
    "        print(\"[Étape 3] Sélection des\", self.n_features, \"meilleures features brutes...\")\n",
    "        top_feats = score_series.head(self.n_features).index.tolist()\n",
    "\n",
    "        # print(\"[Étape 4] Suppression des features corrélées (>\", self.corr_threshold, \")...\")\n",
    "        # reduced_feats = self._remove_correlated(X_df[top_feats], top_feats, threshold=self.corr_threshold)\n",
    "        self.selected_features_stage1 = top_feats[:self.n_features]\n",
    "        print(\"[Résultat] Features sélectionnées (étape 1) :\", len(self.selected_features_stage1))\n",
    "        return self.selected_features_stage1\n",
    "        \n",
    "\n",
    "    def _evaluate_who(self, X, individual):\n",
    "        selected = np.where(individual == 1)[0]\n",
    "        if len(selected) < 2:\n",
    "            return np.inf  # Penalize small subsets\n",
    "\n",
    "        subset = X[:, selected]\n",
    "        try:\n",
    "            # 1. IsolationForest-based clustering\n",
    "            iso_labels = IsolationForest(random_state=0).fit_predict(subset)\n",
    "            if len(np.unique(iso_labels)) < 2:\n",
    "                return np.inf  # Penalize single-cluster solutions\n",
    "            sil_iso = silhouette_score(subset, iso_labels)\n",
    "\n",
    "            # 2. Noise-based KMeans clustering\n",
    "            nbk_labels, _, _ = noise_based_kmeans(subset)\n",
    "            if len(np.unique(nbk_labels)) < 2:\n",
    "                return np.inf\n",
    "            sil_nbk = silhouette_score(subset, nbk_labels)\n",
    "\n",
    "            # Combine the two silhouette scores\n",
    "            avg_sil = (sil_iso + sil_nbk) / 2.0\n",
    "\n",
    "            return -avg_sil + 0.001 * len(selected)  # Minimize loss = -score + complexity_penalty\n",
    "\n",
    "        except Exception as e:\n",
    "            return np.inf\n",
    "\n",
    "    def optimize_with_pso(self, X, n_particles=40, n_iterations=20, inertia=0.7, cognitive=1.4, social=1.4):\n",
    "            print(\"[PSO] Optimisation via Particle Swarm Optimization\")\n",
    "            feature_indices = [self.all_features.index(f) for f in self.selected_features_stage1]\n",
    "            n_features = len(feature_indices)\n",
    "            X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "            X_sel = X_np[:, feature_indices].astype(np.float32)\n",
    "\n",
    "\n",
    "            # Initialize particle positions and velocities\n",
    "            particles = np.random.rand(n_particles, n_features)\n",
    "            velocities = np.random.randn(n_particles, n_features) * 0.1\n",
    "            particles = (particles > 0.5).astype(int)\n",
    "\n",
    "            personal_best = particles.copy()\n",
    "            personal_best_scores = np.array([self._evaluate_who(X_sel, p) for p in particles])\n",
    "\n",
    "            global_best = personal_best[np.argmin(personal_best_scores)].copy()\n",
    "            global_best_score = np.min(personal_best_scores)\n",
    "\n",
    "            for _ in tqdm(range(n_iterations), desc=\"PSO Iterations\"):\n",
    "                for i in range(n_particles):\n",
    "                    r1, r2 = np.random.rand(), np.random.rand()\n",
    "                    velocities[i] = (\n",
    "                        inertia * velocities[i] +\n",
    "                        cognitive * r1 * (personal_best[i] - particles[i]) +\n",
    "                        social * r2 * (global_best - particles[i])\n",
    "                    )\n",
    "                    sigmoid = 1 / (1 + np.exp(-velocities[i]))\n",
    "                    particles[i] = (np.random.rand(n_features) < sigmoid).astype(int)\n",
    "\n",
    "                    score = self._evaluate_who(X_sel, particles[i])\n",
    "                    if score < personal_best_scores[i]:\n",
    "                        personal_best[i] = particles[i].copy()\n",
    "                        personal_best_scores[i] = score\n",
    "                        if score < global_best_score:\n",
    "                            global_best = particles[i].copy()\n",
    "                            global_best_score = score\n",
    "                print(f\"[PSO] {len(self.selected_features_final)} features selected | Best score: {global_best_score:.4f}\")\n",
    "            self.selected_features_final = [\n",
    "                self.selected_features_stage1[i]\n",
    "                for i in range(n_features) if global_best[i] == 1\n",
    "            ]\n",
    "            print(f\"[PSO] {len(self.selected_features_final)} features selected | Best score: {global_best_score:.4f}\")\n",
    "            return self.selected_features_final\n",
    "\n",
    "    def transform_final(self, df):\n",
    "        # Use the same preprocessing as in _prepare_data\n",
    "        X_processed, _ = self._prepare_data(df)\n",
    "        \n",
    "        # Get indices of selected features\n",
    "        feat_idx = [self.all_features.index(f) for f in self.selected_features_final]\n",
    "        \n",
    "        # Return only the selected columns\n",
    "        return X_processed[:, feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7757dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Étape 1] Préparation des données...\n",
      "[Étape 2] Calcul des scores de features non supervisés...\n",
      "[Étape 3] Sélection des 40 meilleures features brutes...\n",
      "[Résultat] Features sélectionnées (étape 1) : 40\n",
      "Top features sélectionnées sans corrélation : ['Birth_date', 'Last_Call_Date', 'First_Call_Date', 'reactivite_client', 'status_date', 'Document_Validation_Date', 'full_date', 'DOC_SCN_DT', 'temps_moyen_appel', 'Activation_Date', 'id_date', 'jours', 'pdv_sk', 'localisation_sk', 'lib_jour', 'temps_moyen_traitement', 'ARPU_Last_2_Months', 'year', 'doc_scan_avant_activation', 'Province', 'PoS_ID', 'Postal_ID', 'Revenue_Last_2_Months', 'Revenue_Last_12_Months', 'DOB_ok', 'tarrif_profile', 'lib_mois', 'ID_doc', 'type_d_operation', 'Street', 'id_type', 'NIN_ok', 'id_operation', 'client_haut_revenue', 'age_sub', 'minor_ok', 'ocr_violation', 'similarity_score', 'similarity_score_bin', 'cn_valid']\n",
      "[PSO] Optimisation via Particle Swarm Optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:   5%|▌         | 1/20 [00:48<15:15, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.3898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  10%|█         | 2/20 [01:36<14:26, 48.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.3898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  15%|█▌        | 3/20 [02:25<13:46, 48.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.3898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  20%|██        | 4/20 [03:14<13:03, 48.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.3947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  25%|██▌       | 5/20 [04:04<12:15, 49.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  30%|███       | 6/20 [04:53<11:30, 49.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  35%|███▌      | 7/20 [05:43<10:39, 49.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  40%|████      | 8/20 [06:32<09:51, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  45%|████▌     | 9/20 [07:46<10:27, 57.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  50%|█████     | 10/20 [09:16<11:13, 67.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  55%|█████▌    | 11/20 [10:39<10:47, 71.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  60%|██████    | 12/20 [11:56<09:47, 73.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  65%|██████▌   | 13/20 [12:44<07:39, 65.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  70%|███████   | 14/20 [13:31<06:01, 60.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  75%|███████▌  | 15/20 [14:20<04:44, 56.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  80%|████████  | 16/20 [15:08<03:36, 54.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  85%|████████▌ | 17/20 [16:14<02:53, 57.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  90%|█████████ | 18/20 [17:38<02:11, 65.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations:  95%|█████████▌| 19/20 [19:02<01:10, 70.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSO Iterations: 100%|██████████| 20/20 [20:24<00:00, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSO] 0 features selected | Best score: -0.4622\n",
      "[PSO] 17 features selected | Best score: -0.4622\n",
      "Features finales sélectionnées par PSO : ['reactivite_client', 'Document_Validation_Date', 'temps_moyen_appel', 'id_date', 'jours', 'localisation_sk', 'lib_jour', 'temps_moyen_traitement', 'doc_scan_avant_activation', 'PoS_ID', 'Revenue_Last_12_Months', 'tarrif_profile', 'type_d_operation', 'id_type', 'NIN_ok', 'id_operation', 'similarity_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "selector = FeatureSelectorForAnomaly(n_features=40)\n",
    "selected_stage1 = selector.select_top_features(df)\n",
    "\n",
    "print(\"Top features sélectionnées sans corrélation :\", selected_stage1)\n",
    "\n",
    "# WHO sur les 40\n",
    "X_all, features = selector._prepare_data(df)\n",
    "selected_final = selector.optimize_with_pso(X_all)\n",
    "\n",
    "\n",
    "print(\"Features finales sélectionnées par PSO :\", selected_final)\n",
    "\n",
    "X_final = selector.transform_final(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90abb4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Lancement de 729 combinaisons avec joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 57.5min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 80.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 122.1min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 141.7min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 225.3min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 288.2min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 336.3min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 377.5min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 490.0min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 589.9min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 744.9min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 800.2min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed: 912.9min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 1107.8min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 1258.9min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 1325.0min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 1414.5min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 1538.8min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 1630.7min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 1869.8min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 2133.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 2254.6min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 2490.3min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 2694.9min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 2782.8min\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed: 2951.6min\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed: 3079.9min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 3421.9min\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed: 3654.7min\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed: 3859.0min\n",
      "[Parallel(n_jobs=-1)]: Done 729 out of 729 | elapsed: 4149.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Résultats enregistrés dans 'feature_selector_experiments_joblib.csv'\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "def run_experiment(params):\n",
    "    try:\n",
    "        n_feat, n_particles, n_iterations, inertia, cognitive, social= params\n",
    "        print(f\"▶️ Running: n_features={n_feat}, n_particles={n_particles}, n_iterations={n_iterations}, inertia={inertia}, mutation_rate={cognitive}, social={social}\")\n",
    "        \n",
    "        # Recharge les données dans chaque worker (évite les problèmes de mémoire partagée)\n",
    "        # df = pd.read_csv(\"your_data.csv\")  # Assurez-vous que le chemin est correct\n",
    "        \n",
    "        selector = FeatureSelectorForAnomaly(n_features=n_feat)\n",
    "        stage1_feats = selector.select_top_features(df)\n",
    "        X_all, features = selector._prepare_data(df)\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        final_feats, score = selector.optimize_with_pso(\n",
    "            X_all, n_particles=n_particles, n_iterations=n_iterations,\n",
    "            inertia=inertia, cognitive=cognitive, social=social\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'n_features': n_feat,\n",
    "            'n_particles': n_particles,\n",
    "            'n_iterations': n_iterations,\n",
    "            'inertia': inertia,\n",
    "            'cognitive': cognitive,\n",
    "            'social': social,\n",
    "            'stage1_selected': len(stage1_feats),\n",
    "            'final_selected': len(final_feats),\n",
    "            'selected_features': list(final_feats),\n",
    "            'score': float(score),\n",
    "            'duration_sec': round(duration, 2)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[❌ Erreur] {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'n_features': n_feat,\n",
    "            'n_particles': n_particles,\n",
    "            'n_iterations': n_iterations,\n",
    "            'inertia': inertia,\n",
    "            'cognitive': cognitive,\n",
    "            'social': social,\n",
    "            'stage1_selected': 0,\n",
    "            'final_selected': 0,\n",
    "            'selected_features': [],\n",
    "            'score': 'error',\n",
    "            'duration_sec': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_features_list = [20, 30, 40]\n",
    "    n_particles = [20, 30, 40]\n",
    "    n_iterations =  [10, 20, 30]\n",
    "    inertia = [0.4, 0.7, 0.9]\n",
    "    cognitive = [1.4, 2.0, 2.5]\n",
    "    social = [1.4, 2.0, 2.5]\n",
    "\n",
    "    param_combinations = list(itertools.product(\n",
    "        n_features_list, n_particles, n_iterations, inertia, cognitive, social\n",
    "        \n",
    "    ))\n",
    "\n",
    "    print(f\"🧠 Lancement de {len(param_combinations)} combinaisons avec joblib...\")\n",
    "\n",
    "    # Utilisation de joblib.Parallel\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(  # n_jobs=-1 utilise tous les cœurs\n",
    "        delayed(run_experiment)(params) for params in param_combinations\n",
    "    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('feature_selector_experiments_joblib_pso.csv', index=False)\n",
    "    print(\"✅ Résultats enregistrés dans 'feature_selector_experiments_joblib.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
